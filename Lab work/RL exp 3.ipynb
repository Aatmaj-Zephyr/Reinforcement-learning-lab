{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":723,"status":"ok","timestamp":1724063615086,"user":{"displayName":"Aatmaj Mhatre","userId":"01202458881510784135"},"user_tz":-330},"id":"TJ7OGc8deK1l","outputId":"df611d89-a859-4d57-85af-060124e75db7"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["import gym\n","\n","# Create the Blackjack environment\n","env = gym.make('Blackjack-v1')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"PpQs0Z7AULY7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724063615554,"user_tz":-330,"elapsed":2,"user":{"displayName":"Aatmaj Mhatre","userId":"01202458881510784135"}},"outputId":"ca2c3467-02a6-4a49-8f0f-752133ccdc39"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["def basic_policy(state):\n","    \"\"\"\n","    A simple policy that hits if the player's sum is less than 20, else holds.\n","    \"\"\"\n","    player_sum, dealer_sum, usable_ace = state\n","    return 0 if player_sum >= 20 else 1  # 0 = hold, 1 = hit\n","\n","def play_episode(policy, env):\n","    \"\"\"\n","    Plays an episode of Blackjack using the given policy.\n","\n","    Args:\n","        policy: A function that takes a state and returns an action (0=hold, 1=hit).\n","        env: The Blackjack environment.\n","\n","    Returns:\n","        A tuple (states, actions, rewards) for the episode.\n","    \"\"\"\n","    states = []\n","    actions = []\n","    rewards = []\n","\n","    state = env.reset()\n","    while True:\n","        states.append(state)\n","        action = policy(state)\n","        actions.append(action)\n","        next_state, reward, done, _ = env.step(action)\n","        rewards.append(reward)\n","        state = next_state\n","        if done:\n","            break\n","\n","    return states, actions, rewards\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1113,"status":"ok","timestamp":1724063616666,"user":{"displayName":"Aatmaj Mhatre","userId":"01202458881510784135"},"user_tz":-330},"id":"O2U8-QPaXChW","outputId":"bdfc6183-6b89-40f1-c61b-3e63008ca646"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]}],"source":["from collections import defaultdict\n","\n","def monte_carlo_policy_evaluation(policy, env, num_episodes, discount_factor=1.0):\n","    \"\"\"\n","    Evaluate a policy using Monte Carlo sampling.\n","\n","    Args:\n","        policy: A function that takes a state and returns an action.\n","        env: The environment.\n","        num_episodes: Number of episodes to sample.\n","        discount_factor: Discount factor for future rewards.\n","\n","    Returns:\n","        A dictionary mapping state to value.\n","    \"\"\"\n","    # Store returns for each state\n","    returns_sum = defaultdict(float)\n","    returns_count = defaultdict(int)\n","    V = defaultdict(float)\n","\n","    for _ in range(num_episodes):\n","        # Generate an episode using the policy\n","        episode = play_episode(policy, env)\n","        states, _, rewards = episode\n","\n","        # Calculate returns\n","        G = 0\n","        for t in reversed(range(len(states))):\n","            G = rewards[t] + discount_factor * G\n","            state = states[t]\n","            # First visit Monte Carlo: only consider first time state is visited in episode\n","            if state not in states[:t]:\n","                returns_sum[state] += G\n","                returns_count[state] += 1\n","                V[state] = returns_sum[state] / returns_count[state]\n","\n","    return V\n","\n","# Example usage: Evaluate the initial policy with many episodes\n","V = monte_carlo_policy_evaluation(basic_policy, env, num_episodes=5000)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724063616666,"user":{"displayName":"Aatmaj Mhatre","userId":"01202458881510784135"},"user_tz":-330},"id":"TkEvWV2hX5yx","outputId":"b4c54b31-89e8-4d43-df77-c610bea58bb8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(float,\n","            {(15, 3, False): -0.5576923076923077,\n","             (17, 3, True): -0.7272727272727273,\n","             (14, 5, False): -0.6666666666666666,\n","             (18, 4, False): -0.711864406779661,\n","             (12, 4, False): -0.6382978723404256,\n","             (8, 4, False): -0.4666666666666667,\n","             (21, 6, False): 0.9574468085106383,\n","             (15, 6, False): -0.4888888888888889,\n","             (19, 5, False): -0.7454545454545455,\n","             (19, 6, False): -0.603448275862069,\n","             (21, 7, True): 1.0,\n","             (20, 6, False): 0.7808219178082192,\n","             (13, 6, False): -0.5476190476190477,\n","             (19, 10, False): -0.7702702702702703,\n","             (18, 10, False): -0.7272727272727273,\n","             (8, 10, False): -0.6206896551724138,\n","             (20, 9, True): 0.9230769230769231,\n","             (16, 7, False): -0.7307692307692307,\n","             (20, 3, False): 0.6865671641791045,\n","             (16, 3, False): -0.6363636363636364,\n","             (21, 10, True): 0.863013698630137,\n","             (17, 8, False): -0.4727272727272727,\n","             (21, 10, False): 0.881578947368421,\n","             (9, 10, False): -0.47619047619047616,\n","             (20, 10, False): 0.4840989399293286,\n","             (17, 10, False): -0.7788018433179723,\n","             (12, 10, False): -0.5755813953488372,\n","             (16, 2, False): -0.875,\n","             (13, 9, False): -0.625,\n","             (14, 10, False): -0.5980392156862745,\n","             (18, 7, False): -0.6875,\n","             (13, 1, False): -0.825,\n","             (12, 1, False): -0.6071428571428571,\n","             (21, 2, False): 0.85,\n","             (14, 2, False): -0.65625,\n","             (19, 6, True): -0.5555555555555556,\n","             (8, 6, False): -0.7142857142857143,\n","             (16, 10, False): -0.702020202020202,\n","             (15, 8, False): -0.723404255319149,\n","             (17, 8, True): -0.5,\n","             (16, 8, True): -0.42857142857142855,\n","             (21, 3, False): 0.8333333333333334,\n","             (19, 3, False): -0.627906976744186,\n","             (19, 1, False): -0.88,\n","             (19, 1, True): -0.5,\n","             (18, 5, False): -0.7446808510638298,\n","             (19, 8, False): -0.509090909090909,\n","             (10, 8, False): -0.2,\n","             (17, 6, False): -0.6304347826086957,\n","             (16, 6, False): -0.6326530612244898,\n","             (9, 6, False): 0.0,\n","             (18, 1, False): -0.9019607843137255,\n","             (17, 7, False): -0.8043478260869565,\n","             (20, 5, False): 0.6309523809523809,\n","             (10, 5, False): -0.18181818181818182,\n","             (19, 10, True): -0.6666666666666666,\n","             (17, 1, False): -0.7333333333333333,\n","             (11, 1, False): -0.32432432432432434,\n","             (10, 10, False): -0.06896551724137931,\n","             (20, 9, False): 0.7142857142857143,\n","             (10, 9, False): 0.0,\n","             (17, 4, False): -0.6666666666666666,\n","             (15, 10, False): -0.6401869158878505,\n","             (17, 10, True): -0.5357142857142857,\n","             (11, 10, False): -0.275,\n","             (14, 4, False): -0.5454545454545454,\n","             (14, 4, True): -0.2,\n","             (21, 8, False): 0.8,\n","             (11, 8, False): -0.1111111111111111,\n","             (16, 5, False): -0.5161290322580645,\n","             (12, 8, False): -0.325,\n","             (13, 10, False): -0.6481481481481481,\n","             (14, 7, False): -0.6296296296296297,\n","             (7, 7, False): -0.6666666666666666,\n","             (20, 1, False): 0.13793103448275862,\n","             (16, 8, False): -0.7368421052631579,\n","             (19, 7, False): -0.711864406779661,\n","             (19, 7, True): -0.5,\n","             (16, 7, True): 0.3333333333333333,\n","             (14, 8, False): -0.8292682926829268,\n","             (15, 7, False): -0.5,\n","             (9, 7, False): -0.7333333333333333,\n","             (19, 2, False): -0.8461538461538461,\n","             (11, 2, False): -0.45454545454545453,\n","             (15, 4, False): -0.5833333333333334,\n","             (13, 3, False): -0.6444444444444445,\n","             (11, 3, False): -0.38461538461538464,\n","             (15, 1, False): -0.7647058823529411,\n","             (20, 2, False): 0.6323529411764706,\n","             (13, 2, False): -0.29411764705882354,\n","             (12, 2, False): -0.5660377358490566,\n","             (12, 2, True): 0.0,\n","             (17, 3, False): -0.6610169491525424,\n","             (12, 3, False): -0.6304347826086957,\n","             (15, 5, False): -0.7708333333333334,\n","             (18, 10, True): -0.5714285714285714,\n","             (7, 4, False): -0.6666666666666666,\n","             (18, 4, True): -0.375,\n","             (21, 1, False): 0.625,\n","             (6, 1, False): -0.5714285714285714,\n","             (19, 4, False): -0.7391304347826086,\n","             (11, 7, False): -0.15384615384615385,\n","             (13, 7, False): -0.5454545454545454,\n","             (12, 5, False): -0.5652173913043478,\n","             (7, 5, False): -0.2,\n","             (18, 6, False): -0.5365853658536586,\n","             (6, 6, False): -0.6666666666666666,\n","             (21, 7, False): 0.9545454545454546,\n","             (17, 5, False): -0.7551020408163265,\n","             (13, 5, False): -0.5,\n","             (18, 3, False): -0.7777777777777778,\n","             (21, 9, True): 0.9615384615384616,\n","             (21, 2, True): 0.9629629629629629,\n","             (21, 4, False): 0.9230769230769231,\n","             (20, 6, True): 0.875,\n","             (9, 4, False): -0.3157894736842105,\n","             (8, 9, False): -0.6842105263157895,\n","             (21, 4, True): 0.9642857142857143,\n","             (20, 8, False): 0.7926829268292683,\n","             (16, 1, False): -0.8431372549019608,\n","             (21, 5, False): 0.875,\n","             (21, 9, False): 0.9090909090909091,\n","             (11, 9, False): -0.03225806451612903,\n","             (7, 9, False): -0.2222222222222222,\n","             (13, 2, True): -0.25,\n","             (15, 9, True): 0.0,\n","             (7, 3, False): -0.5,\n","             (12, 6, False): -0.41509433962264153,\n","             (14, 10, True): -0.3,\n","             (8, 7, False): -1.0,\n","             (15, 9, False): -0.6607142857142857,\n","             (18, 9, True): -0.3333333333333333,\n","             (17, 9, True): -0.14285714285714285,\n","             (20, 7, False): 0.8142857142857143,\n","             (18, 2, False): -0.7313432835820896,\n","             (8, 2, False): -0.5714285714285714,\n","             (12, 7, False): -0.5531914893617021,\n","             (16, 4, False): -0.6666666666666666,\n","             (18, 9, False): -0.6666666666666666,\n","             (14, 9, False): -0.48936170212765956,\n","             (12, 10, True): -0.7,\n","             (19, 9, False): -0.6964285714285714,\n","             (11, 6, False): -0.08,\n","             (16, 5, True): -0.6666666666666666,\n","             (15, 5, True): -0.7142857142857143,\n","             (4, 5, False): -1.0,\n","             (6, 4, False): -0.5,\n","             (9, 3, False): -0.7777777777777778,\n","             (17, 5, True): -0.7142857142857143,\n","             (17, 6, True): -0.3333333333333333,\n","             (9, 9, False): -0.5,\n","             (6, 10, False): -0.68,\n","             (10, 4, False): -0.7333333333333333,\n","             (15, 2, False): -0.5909090909090909,\n","             (16, 2, True): -0.4444444444444444,\n","             (21, 1, True): 0.6666666666666666,\n","             (14, 3, False): -0.875,\n","             (15, 2, True): -1.0,\n","             (21, 3, True): 0.9545454545454546,\n","             (21, 6, True): 1.0,\n","             (14, 1, False): -0.6739130434782609,\n","             (12, 9, False): -0.717391304347826,\n","             (15, 10, True): -0.5,\n","             (13, 10, True): -0.2,\n","             (18, 3, True): -0.5,\n","             (14, 2, True): 0.0,\n","             (20, 4, False): 0.6197183098591549,\n","             (13, 4, False): -0.6862745098039216,\n","             (10, 6, False): 0.0,\n","             (11, 4, False): -0.07142857142857142,\n","             (20, 7, True): 1.0,\n","             (15, 7, True): -0.09090909090909091,\n","             (17, 9, False): -0.6206896551724138,\n","             (16, 9, False): -0.5344827586206896,\n","             (6, 7, False): -0.8333333333333334,\n","             (18, 8, False): -0.5178571428571429,\n","             (8, 8, False): -0.4117647058823529,\n","             (5, 10, False): -0.2,\n","             (7, 10, False): -0.7073170731707317,\n","             (17, 2, False): -0.6486486486486487,\n","             (17, 4, True): -0.8,\n","             (7, 8, False): -0.1111111111111111,\n","             (13, 8, False): -0.6296296296296297,\n","             (16, 1, True): -0.3333333333333333,\n","             (8, 5, False): -0.42857142857142855,\n","             (6, 9, False): -0.45454545454545453,\n","             (17, 1, True): -0.875,\n","             (21, 8, True): 0.9523809523809523,\n","             (5, 4, False): -1.0,\n","             (4, 9, False): 0.0,\n","             (15, 1, True): -0.3333333333333333,\n","             (16, 3, True): -0.7142857142857143,\n","             (18, 7, True): 0.0,\n","             (6, 8, False): 0.3333333333333333,\n","             (20, 10, True): 0.425531914893617,\n","             (13, 4, True): -0.5,\n","             (20, 4, True): 0.6153846153846154,\n","             (4, 10, False): -0.8,\n","             (11, 5, False): -0.25,\n","             (10, 7, False): 0.19047619047619047,\n","             (5, 7, False): -0.14285714285714285,\n","             (7, 2, False): -0.5,\n","             (17, 7, True): -0.42857142857142855,\n","             (10, 1, False): -0.4,\n","             (9, 2, False): -0.38461538461538464,\n","             (15, 8, True): -0.16666666666666666,\n","             (5, 1, False): 0.0,\n","             (10, 3, False): 0.11764705882352941,\n","             (4, 7, False): -1.0,\n","             (14, 6, False): -0.3333333333333333,\n","             (14, 6, True): -0.3333333333333333,\n","             (8, 1, False): -0.4,\n","             (14, 9, True): 0.25,\n","             (19, 3, True): -0.5,\n","             (15, 3, True): 0.0,\n","             (16, 4, True): 0.0,\n","             (19, 2, True): -1.0,\n","             (21, 5, True): 0.9545454545454546,\n","             (10, 2, False): 0.125,\n","             (5, 2, False): -0.3333333333333333,\n","             (16, 10, True): -0.2222222222222222,\n","             (9, 1, False): -0.6875,\n","             (20, 2, True): 0.6666666666666666,\n","             (18, 2, True): -0.2,\n","             (20, 5, True): 0.7142857142857143,\n","             (14, 5, True): 0.0,\n","             (19, 5, True): -0.15384615384615385,\n","             (14, 3, True): -0.3333333333333333,\n","             (9, 8, False): -0.125,\n","             (7, 1, False): -0.5,\n","             (16, 6, True): -0.5454545454545454,\n","             (15, 6, True): 0.0,\n","             (18, 5, True): -0.1111111111111111,\n","             (20, 1, True): -0.2222222222222222,\n","             (19, 4, True): -0.2,\n","             (15, 4, True): -1.0,\n","             (13, 9, True): -0.3333333333333333,\n","             (8, 3, False): -0.75,\n","             (6, 3, False): -0.7777777777777778,\n","             (4, 4, False): -0.6,\n","             (12, 6, True): 0.3333333333333333,\n","             (14, 8, True): 0.6,\n","             (16, 9, True): -1.0,\n","             (13, 6, True): 0.16666666666666666,\n","             (7, 6, False): -0.5714285714285714,\n","             (5, 8, False): -0.6,\n","             (18, 1, True): -0.3,\n","             (20, 3, True): 0.8,\n","             (19, 8, True): 0.14285714285714285,\n","             (18, 8, True): 0.1111111111111111,\n","             (4, 6, False): 0.0,\n","             (13, 8, True): 1.0,\n","             (6, 2, False): -1.0,\n","             (20, 8, True): 1.0,\n","             (9, 5, False): -0.8461538461538461,\n","             (13, 1, True): -0.6,\n","             (18, 6, True): -0.3333333333333333,\n","             (19, 9, True): -0.5555555555555556,\n","             (4, 3, False): -1.0,\n","             (13, 7, True): 0.3333333333333333,\n","             (4, 8, False): -0.5,\n","             (14, 7, True): -0.3333333333333333,\n","             (4, 1, False): -0.3333333333333333,\n","             (12, 8, True): 1.0,\n","             (6, 5, False): -1.0,\n","             (5, 5, False): -0.6,\n","             (4, 2, False): -1.0,\n","             (12, 3, True): 0.0,\n","             (17, 2, True): 0.16666666666666666,\n","             (12, 7, True): 1.0,\n","             (12, 1, True): -0.75,\n","             (13, 3, True): -1.0,\n","             (5, 9, False): -1.0,\n","             (5, 3, False): -0.5,\n","             (14, 1, True): 0.0,\n","             (12, 9, True): 0.0,\n","             (12, 5, True): 0.3333333333333333,\n","             (12, 4, True): 0.5})"]},"metadata":{},"execution_count":5}],"source":["V"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"fzBhZuroYovI","executionInfo":{"status":"ok","timestamp":1724063616666,"user_tz":-330,"elapsed":2,"user":{"displayName":"Aatmaj Mhatre","userId":"01202458881510784135"}}},"outputs":[],"source":["def greedy_policy_from_value_function(V, env):\n","    \"\"\"\n","    Create a greedy policy based on the given value function.\n","\n","    Args:\n","        V: A dictionary mapping state to value.\n","        env: The environment.\n","\n","    Returns:\n","        A policy function that maps state to action.\n","    \"\"\"\n","    def policy(state):\n","        # Get the player sum from the state\n","        player_sum, _, _ = state\n","        # Greedy policy: hit if V(player_sum + card_value) > V(player_sum), hold otherwise\n","        hit_value = 0\n","        if player_sum < 21:  # If the player sum is less than 21, consider hitting\n","            for card_value in range(1, 11):  # Possible values of the next card\n","                next_state = (player_sum + card_value, state[1], state[2])\n","                if next_state in V:\n","                    hit_value += V[next_state]\n","            hit_value /= 10  # Average over all possible card values\n","\n","        hold_value = V[state] if state in V else 0\n","\n","        return 1 if hit_value > hold_value else 0\n","\n","    return policy\n","\n","# Improve the policy based on the current value function V\n","improved_policy = greedy_policy_from_value_function(V, env)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"7goEolCcY1cO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724063756999,"user_tz":-330,"elapsed":460,"user":{"displayName":"Aatmaj Mhatre","userId":"01202458881510784135"}},"outputId":"eeb78c19-3b92-4085-e080-a0af64187f2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average return of the optimal policy over 1000 episodes: -0.40\n"]}],"source":["def evaluate_policy(policy, env, num_episodes):\n","    \"\"\"\n","    Evaluate the performance of a policy by running multiple episodes.\n","\n","    Args:\n","        policy: A policy function that maps state to action.\n","        env: The environment.\n","        num_episodes: Number of episodes to evaluate.\n","\n","    Returns:\n","        The average return over the episodes.\n","    \"\"\"\n","    total_return = 0\n","\n","    for _ in range(num_episodes):\n","        states, _, rewards = play_episode(policy, env)\n","        total_return += sum(rewards)\n","\n","    return total_return / num_episodes\n","\n","# Evaluate the optimal policy\n","num_test_episodes = 1000\n","average_return = evaluate_policy(improved_policy, env, num_test_episodes)\n","\n","print(f\"Average return of the optimal policy over {num_test_episodes} episodes: {average_return:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olD0ZF-bX6eL","outputId":"eeb42a7e-59bc-402b-fd5f-79b5872126eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration number 1 trained model for 500000 episodes, Tested for 1000 has average return -0.40\n","Iteration number 2 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 3 trained model for 500000 episodes, Tested for 1000 has average return -0.43\n","Iteration number 4 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 5 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 6 trained model for 500000 episodes, Tested for 1000 has average return -0.44\n","Iteration number 7 trained model for 500000 episodes, Tested for 1000 has average return -0.42\n","Iteration number 8 trained model for 500000 episodes, Tested for 1000 has average return -0.42\n","Iteration number 9 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 10 trained model for 500000 episodes, Tested for 1000 has average return -0.34\n","Iteration number 11 trained model for 500000 episodes, Tested for 1000 has average return -0.40\n","Iteration number 12 trained model for 500000 episodes, Tested for 1000 has average return -0.33\n","Iteration number 13 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 14 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 15 trained model for 500000 episodes, Tested for 1000 has average return -0.40\n","Iteration number 16 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 17 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 18 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 19 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 20 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 21 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 22 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 23 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 24 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 25 trained model for 500000 episodes, Tested for 1000 has average return -0.42\n","Iteration number 26 trained model for 500000 episodes, Tested for 1000 has average return -0.36\n","Iteration number 27 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 28 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 29 trained model for 500000 episodes, Tested for 1000 has average return -0.32\n","Iteration number 30 trained model for 500000 episodes, Tested for 1000 has average return -0.40\n","Iteration number 31 trained model for 500000 episodes, Tested for 1000 has average return -0.34\n","Iteration number 32 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 33 trained model for 500000 episodes, Tested for 1000 has average return -0.42\n","Iteration number 34 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 35 trained model for 500000 episodes, Tested for 1000 has average return -0.40\n","Iteration number 36 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 37 trained model for 500000 episodes, Tested for 1000 has average return -0.33\n","Iteration number 38 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 39 trained model for 500000 episodes, Tested for 1000 has average return -0.43\n","Iteration number 40 trained model for 500000 episodes, Tested for 1000 has average return -0.40\n","Iteration number 41 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 42 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 43 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 44 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 45 trained model for 500000 episodes, Tested for 1000 has average return -0.40\n","Iteration number 46 trained model for 500000 episodes, Tested for 1000 has average return -0.33\n","Iteration number 47 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 48 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 49 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 50 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 51 trained model for 500000 episodes, Tested for 1000 has average return -0.44\n","Iteration number 52 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 53 trained model for 500000 episodes, Tested for 1000 has average return -0.40\n","Iteration number 54 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 55 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 56 trained model for 500000 episodes, Tested for 1000 has average return -0.43\n","Iteration number 57 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 58 trained model for 500000 episodes, Tested for 1000 has average return -0.42\n","Iteration number 59 trained model for 500000 episodes, Tested for 1000 has average return -0.40\n","Iteration number 60 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 61 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 62 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 63 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 64 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 65 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 66 trained model for 500000 episodes, Tested for 1000 has average return -0.36\n","Iteration number 67 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 68 trained model for 500000 episodes, Tested for 1000 has average return -0.36\n","Iteration number 69 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 70 trained model for 500000 episodes, Tested for 1000 has average return -0.36\n","Iteration number 71 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 72 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 73 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 74 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 75 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 76 trained model for 500000 episodes, Tested for 1000 has average return -0.32\n","Iteration number 77 trained model for 500000 episodes, Tested for 1000 has average return -0.44\n","Iteration number 78 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 79 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 80 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 81 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 82 trained model for 500000 episodes, Tested for 1000 has average return -0.36\n","Iteration number 83 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 84 trained model for 500000 episodes, Tested for 1000 has average return -0.37\n","Iteration number 85 trained model for 500000 episodes, Tested for 1000 has average return -0.38\n","Iteration number 86 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 87 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 88 trained model for 500000 episodes, Tested for 1000 has average return -0.36\n","Iteration number 89 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 90 trained model for 500000 episodes, Tested for 1000 has average return -0.32\n","Iteration number 91 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 92 trained model for 500000 episodes, Tested for 1000 has average return -0.39\n","Iteration number 93 trained model for 500000 episodes, Tested for 1000 has average return -0.41\n","Iteration number 94 trained model for 500000 episodes, Tested for 1000 has average return -0.44\n","Iteration number 95 trained model for 500000 episodes, Tested for 1000 has average return -0.35\n","Iteration number 96 trained model for 500000 episodes, Tested for 1000 has average return -0.44\n","Iteration number 97 trained model for 500000 episodes, Tested for 1000 has average return -0.36\n"]}],"source":["def monte_carlo_policy_iteration(env, num_episodes, discount_factor=0.9, tol=1e-10):\n","    \"\"\"\n","    Perform Monte Carlo Policy Iteration to find an optimal policy.\n","\n","    Args:\n","        env: The environment.\n","        num_episodes: Number of episodes to sample for policy evaluation.\n","        discount_factor: Discount factor for future rewards.\n","        tol: Convergence tolerance for policy improvement.\n","\n","    Returns:\n","        A tuple (optimal_policy, optimal_value_function).\n","    \"\"\"\n","    # Initialize a random policy\n","    policy = basic_policy  # initial policy\n","    V = defaultdict(float)\n","    counter = 0\n","    while True:\n","        counter += 1\n","\n","        average_return = evaluate_policy(improved_policy, env, num_test_episodes)\n","\n","        print(f\"Iteration number {counter} trained model for {num_episodes} episodes, Tested for {num_test_episodes} has average return {average_return:.2f}\")\n","        if(counter >100):\n","          break\n","        # Policy Evaluation\n","        V_new = monte_carlo_policy_evaluation(policy, env, num_episodes, discount_factor)\n","\n","        # Policy Improvement\n","        new_policy = greedy_policy_from_value_function(V_new, env)\n","\n","\n","\n","        # Check for convergence\n","        if max(abs(V_new[state] - V[state]) for state in V_new) < tol:\n","            break\n","\n","\n","        # Update policy and value function\n","        policy = new_policy\n","        V = V_new\n","\n","    return policy, V\n","\n","# Perform Monte Carlo Policy Iteration\n","improved_policy, optimal_value_function = monte_carlo_policy_iteration(env, num_episodes=500000)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7/4X1ulhEShQmPkMkOmJ4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}